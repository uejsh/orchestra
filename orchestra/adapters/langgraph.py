# langgraph.py - LangGraph enhancement
# ============================================================================
# FILE: orchestra/adapters/langgraph.py
# LangGraph adapter - Drop-in semantic caching enhancement
# ============================================================================

import time
import hashlib
import json
import logging
from typing import Any, Dict, Optional, Callable
from functools import wraps

from ..core.metrics import MetricsTracker

logger = logging.getLogger(__name__)


class OrchestraConfig:
    """Configuration for Orchestra enhancement"""
    
    def __init__(
        self,
        # Semantic matching
        similarity_threshold: float = 0.92,
        embedding_model: str = "all-MiniLM-L6-v2",
        
        # Hierarchical embeddings (multi-level matching)
        enable_hierarchical: bool = False,
        hierarchical_weight_l1: float = 0.6,  # Weight for full query similarity
        hierarchical_weight_l2: float = 0.4,  # Weight for chunk similarity
        
        # Caching
        cache_ttl: int = 3600,
        max_cache_size: int = 10000,
        enable_cache: bool = True,
        
        # Compression
        enable_compression: bool = False,
        
        # Cost tracking
        llm_cost_per_1k_tokens: float = 0.03,
        
        # Advanced
        auto_cleanup: bool = True,
        cleanup_interval: int = 300,  # 5 minutes
        
        # Backend
        redis_url: Optional[str] = None,  # e.g., "redis://localhost:6379"
        
        # Observability
        enable_recorder: bool = True,
        
        # Resilience
        enable_circuit_breaker: bool = False,
        circuit_breaker_threshold: int = 5,
        circuit_breaker_timeout: float = 60.0,
        
        # MCP Integration (Smart Tool Discovery)
        mcp_servers: Optional[list] = None,  # List of MCPConfig or connection strings
        enable_tool_search: bool = True,  # Like Claude's Tool Search
        tool_search_top_k: int = 5,  # Max tools when searching
        tool_context_threshold: float = 0.10,  # 10% of context triggers search
        mcp_cache_ttl: int = 3600  # TTL for MCP tool responses
    ):
        self.similarity_threshold = similarity_threshold
        self.embedding_model = embedding_model
        self.enable_hierarchical = enable_hierarchical
        self.hierarchical_weight_l1 = hierarchical_weight_l1
        self.hierarchical_weight_l2 = hierarchical_weight_l2
        self.cache_ttl = cache_ttl
        self.max_cache_size = max_cache_size
        self.enable_cache = enable_cache
        self.enable_compression = enable_compression
        self.llm_cost_per_1k_tokens = llm_cost_per_1k_tokens
        self.auto_cleanup = auto_cleanup
        self.cleanup_interval = cleanup_interval
        self.redis_url = redis_url
        self.enable_recorder = enable_recorder
        self.enable_circuit_breaker = enable_circuit_breaker
        self.circuit_breaker_threshold = circuit_breaker_threshold
        self.circuit_breaker_timeout = circuit_breaker_timeout
        
        # MCP settings
        self.mcp_servers = mcp_servers
        self.enable_tool_search = enable_tool_search
        self.tool_search_top_k = tool_search_top_k
        self.tool_context_threshold = tool_context_threshold
        self.mcp_cache_ttl = mcp_cache_ttl


class EnhancedLangGraph:
    """
    Wrapper around compiled LangGraph that adds semantic caching.
    
    This is the magic that makes Orchestra work with zero code changes.
    
    Features (configurable via OrchestraConfig):
    - Semantic caching with FAISS/NumPy
    - Hierarchical embeddings for better matching (optional)
    - Compression for smaller cache footprint (optional)
    """
    
    def __init__(
        self,
        graph,  # CompiledGraph from LangGraph
        config: Optional[OrchestraConfig] = None
    ):
        """
        Args:
            graph: Compiled LangGraph instance
            config: Orchestra configuration
        """
        self.graph = graph
        self.config = config or OrchestraConfig()
        
        # ====================================================================
        # UNIFIED CACHE MANAGER (Replaces duplicate component initialization)
        # ====================================================================
        from ..core.cache_manager import CacheManager
        
        self.cache_manager = CacheManager(
            embedding_model_name=self.config.embedding_model,
            similarity_threshold=self.config.similarity_threshold,
            max_cache_size=self.config.max_cache_size,
            cache_ttl=self.config.cache_ttl,
            enable_compression=self.config.enable_compression,
            enable_hierarchical=self.config.enable_hierarchical,
            hierarchical_weight_l1=self.config.hierarchical_weight_l1,
            hierarchical_weight_l2=self.config.hierarchical_weight_l2,
            redis_url=self.config.redis_url
        )
        
        # Metrics
        self.metrics = MetricsTracker(
            llm_cost_per_1k_tokens=self.config.llm_cost_per_1k_tokens
        )
        
        # Last cleanup time
        self._last_cleanup = time.time()
        
        # Recorder
        if self.config.enable_recorder:
            from ..recorder.logger import OrchestraRecorder
            self.recorder = OrchestraRecorder.get_instance()
            logger.info("ðŸŽ¥ Orchestra Recorder ENABLED (LangGraph)")
            self._wrap_nodes()
        else:
            self.recorder = None
        
        logger.info("âœ¨ Orchestra enhancement enabled for LangGraph")
        
        # Resilience
        if self.config.enable_circuit_breaker:
            from ..resilience.circuit_breaker import CircuitBreaker
            self.circuit_breaker = CircuitBreaker(
                failure_threshold=self.config.circuit_breaker_threshold,
                timeout=self.config.circuit_breaker_timeout
            )
            logger.info("ðŸ›¡ï¸ Circuit Breaker ENABLED")
        else:
            self.circuit_breaker = None
    
    def invoke(
        self,
        input: Dict[str, Any],
        config: Optional[Dict] = None,
        use_cache: bool = True,
        **kwargs
    ) -> Any:
        """
        Enhanced invoke with semantic caching.
        
        Args:
            input: Graph input
            config: LangGraph config
            use_cache: Whether to use cache (default: True)
            **kwargs: Additional arguments
        
        Returns:
            Graph output
        """
        start_time = time.time()
        
        # Auto cleanup if needed
        self._maybe_cleanup()
        
        # Generate cache key from input
        input_str = self._serialize_input(input)
        cache_key = self._generate_cache_key(input_str)
        
        # Check semantic cache
        cached_result = None
        if use_cache and self.config.enable_cache:
            cached_result = self._check_cache(input_str, cache_key)
        
        if cached_result is not None:
            # Cache HIT
            latency = time.time() - start_time
            
            self.metrics.record_cache_hit(latency)
            
            logger.info(
                f"ðŸ’° Cache HIT - "
                f"Saved ~${self.metrics.estimate_llm_cost():.2f}, "
                f"Latency: {latency:.3f}s"
            )
            
            # Record trace for cache hit
            if self.recorder:
                with self.recorder.trace(input_params=input, metadata={"cached": True}) as trace_id:
                     self.recorder.finish_trace(trace_id, cached_result, total_cost=0.0)

            return cached_result
        
        # Cache MISS - Execute graph with node-level recording
        logger.debug(f"Cache MISS - Executing graph...")
        
        self.metrics.start_execution()
        
        if self.recorder:
            # Use streaming-based execution for node-level observability
            with self.recorder.trace(input_params=input, metadata={"cached": False, "adapter": "langgraph"}) as trace_id:
                try:
                    if self.circuit_breaker:
                        # Wrap the specialized node recording method
                        result = self.circuit_breaker.call(
                            self._invoke_with_node_recording,
                            input, config, trace_id, **kwargs
                        )
                    else:
                        result = self._invoke_with_node_recording(input, config, trace_id, **kwargs)
                        
                    self.recorder.finish_trace(trace_id, result, total_cost=0.0)
                except Exception as e:
                    # trace context manager handles error status, but we might want to log
                    logger.error(f"Error during graph execution: {e}")
                    raise e
        else:
            if self.circuit_breaker:
                result = self.circuit_breaker.call(self.graph.invoke, input, config, **kwargs)
            else:
                result = self.graph.invoke(input, config, **kwargs)

        
        execution_time = time.time() - start_time
        self.metrics.end_execution(execution_time)
        
        # Store result in semantic cache
        if self.config.enable_cache:
            self._store_result(input_str, result, cache_key)
        
        logger.info(
            f"âš¡ Execution complete - "
            f"Latency: {execution_time:.3f}s"
        )
        
        return result
    
    async def ainvoke(
        self,
        input: Dict[str, Any],
        config: Optional[Dict] = None,
        use_cache: bool = True,
        **kwargs
    ) -> Any:
        """
        Async version of invoke with semantic caching.
        """
        start_time = time.time()
        
        # Auto cleanup
        self._maybe_cleanup()
        
        # Generate cache key
        input_str = self._serialize_input(input)
        cache_key = self._generate_cache_key(input_str)
        
        # Check cache
        cached_result = None
        if use_cache and self.config.enable_cache:
            cached_result = self._check_cache(input_str, cache_key)
        
        if cached_result is not None:
            latency = time.time() - start_time
            self.metrics.record_cache_hit(latency)
            
            logger.info(f"ðŸ’° Cache HIT - Latency: {latency:.3f}s")
            return cached_result
        
        # Execute
        self.metrics.start_execution()
        
        result = await self.graph.ainvoke(input, config, **kwargs)
        
        execution_time = time.time() - start_time
        self.metrics.end_execution(execution_time)
        
        # Store
        if self.config.enable_cache:
            self._store_result(input_str, result, cache_key)
        
        return result
    
    def stream(self, *args, **kwargs):
        """Pass through to original graph (caching not supported for streaming)"""
        logger.warning("Streaming does not support caching - using original graph")
        return self.graph.stream(*args, **kwargs)
    
    def astream(self, *args, **kwargs):
        """Pass through to original graph"""
        logger.warning("Streaming does not support caching - using original graph")
        return self.graph.astream(*args, **kwargs)

    def _invoke_with_node_recording(
        self,
        input: Dict[str, Any],
        config: Optional[Dict],
        trace_id: str,
        **kwargs
    ) -> Any:
        """
        Execute graph using streaming to capture node-level events.
        
        This solves the "blackbox problem" by recording each node's
        input/output states and diffs.
        """
        import asyncio
        from ..recorder.event_listener import LangGraphEventListener
        
        listener = LangGraphEventListener(self.recorder, trace_id)
        
        async def _run_with_events():
            """Async execution using astream_events for node capture."""
            final_output = None
            prev_state = {}
            active_nodes = {}
            
            try:
                async for event in self.graph.astream_events(input, config=config, version="v2", **kwargs):
                    event_type = event.get("event", "")
                    event_name = event.get("name", "")
                    run_id = event.get("run_id", "")
                    data = event.get("data", {})
                    metadata = event.get("metadata", {})
                    
                    # Capture node start events
                    if event_type == "on_chain_start":
                        # Check if this is a graph node (not nested LLM call)
                        langgraph_step = metadata.get("langgraph_step", None)
                        if langgraph_step is not None or "graph" in str(event.get("tags", [])).lower():
                            input_state = data.get("input", {})
                            input_diff = self.recorder.compute_diff(prev_state, input_state)
                            
                            self.recorder.storage.create_step(
                                step_id=run_id,
                                trace_id=trace_id,
                                node_name=event_name,
                                input_state=input_state,
                                input_diff=input_diff if input_diff else None
                            )
                            active_nodes[run_id] = {
                                "name": event_name,
                                "input": input_state,
                                "started": time.time()
                            }
                            logger.debug(f"ðŸ“ Node START: {event_name}")
                    
                    # Capture node end events
                    elif event_type == "on_chain_end":
                        if run_id in active_nodes:
                            node_info = active_nodes.pop(run_id)
                            output_state = data.get("output", {})
                            output_diff = self.recorder.compute_diff(node_info["input"], output_state)
                            
                            self.recorder.storage.update_step(
                                step_id=run_id,
                                output_state=output_state,
                                output_diff=output_diff if output_diff else None,
                                status="SUCCESS",
                                ended_at=time.time()
                            )
                            
                            # Update prev_state for next node
                            if isinstance(output_state, dict):
                                prev_state.update(output_state)
                            
                            duration = time.time() - node_info["started"]
                            logger.debug(f"âœ… Node END: {node_info['name']} ({duration:.3f}s)")
                            
                            final_output = output_state
                            
            except Exception as e:
                logger.error(f"Error during node recording: {e}")
                raise
            
            return final_output
        
        # Run async execution synchronously
        try:
            loop = asyncio.get_running_loop()
            # Already in async context - this shouldn't happen in normal invoke()
            # Fall back to regular invoke
            logger.warning("Already in async context, falling back to regular invoke")
            return self.graph.invoke(input, config, **kwargs)
        except RuntimeError:
            # No running loop - safe to use asyncio.run
            return asyncio.run(_run_with_events())

    def _wrap_nodes(self):
        """
        Wrap all nodes in the underlying graph to capture execution steps.
        This modifies the graph in-place (or the compiled artifact).
        """
        try:
            # LangGraph CompiledGraph stores nodes in .nodes
            # But the underlying Pregel graph is complex.
            # We will attempt to wrap the executables in the graph.nodes dictionary.
            
            nodes = self.graph.nodes
            for node_name, node in nodes.items():
                 # We need to wrap the 'func' or runnable inside the node
                 # LangGraph nodes can be RunnableLambda, etc.
                 # The safest way is to wrap the runnable's invoke/src.
                 
                 # However, since we can't easily modify internal LangGraph structures reliably across versions,
                 # we will wrap the functions if possible or just log from the graph wrapper itself
                 # if we can't deep-inject.
                 
                 # Strategy A: If node is a simple function or RunnableLambda, wrap it.
                 pass # For now, we rely on the graph-level trace. 
                 # To do node-level, we'd need to reconstruct the graph with wrapped nodes.
                 # Given this is "Zero Code Change", we might be limited here without rebuilding the graph.
                 
                 # WAIT! We can use "checkpointer" or "listeners" if LangGraph supports them.
                 # But sticking to the specific task: "Wrap nodes".
                 # This is hard on a *compiled* graph. 
                 
                 # ALTERNATIVE: Wrapper injects itself.
                 # Let's try to wrap the executables in the 'nodes' dict if mutable.
        except Exception as e:
            logger.warning(f"Could not wrap nodes for recording: {e}")

    
    # ========================================================================
    # CACHE OPERATIONS (Delegated to CacheManager)
    # ========================================================================
    
    def _check_cache(self, input_str: str, cache_key: str) -> Optional[Any]:
        """Check semantic cache for similar inputs - delegates to CacheManager"""
        return self.cache_manager.get(input_str)
    
    def _store_result(self, input_str: str, result: Any, cache_key: str):
        """Store result in semantic cache - delegates to CacheManager"""
        self.cache_manager.put(input_str, result, ttl=self.config.cache_ttl)
        logger.debug(f"Stored result in cache: {cache_key[:16]}...")
    
    # ========================================================================
    # UTILITIES
    # ========================================================================
    
    def _serialize_input(self, input: Any) -> str:
        """Serialize input to string for embedding"""
        try:
            return json.dumps(input, sort_keys=True, default=str)
        except:
            return str(input)
    
    def _generate_cache_key(self, input_str: str) -> str:
        """Generate deterministic cache key"""
        return hashlib.sha256(input_str.encode()).hexdigest()
    
    def _hash_input(self, input_str: str) -> str:
        """Hash input for quick comparison"""
        return hashlib.md5(input_str.encode()).hexdigest()
    
    def _maybe_cleanup(self):
        """Cleanup expired cache entries if needed"""
        if not self.config.auto_cleanup:
            return
        
        current_time = time.time()
        if current_time - self._last_cleanup < self.config.cleanup_interval:
            return
        
        removed = self.cache_manager.cleanup_expired()
        if removed > 0:
            logger.info(f"ðŸ—‘ï¸  Auto-cleanup removed {removed} expired entries")
        
        self._last_cleanup = current_time
    
    # ========================================================================
    # PUBLIC API
    # ========================================================================
    
    def invalidate_cache(self, query: Optional[str] = None):
        """
        Invalidate cache entries.
        
        Args:
            query: Specific query to invalidate (None = invalidate all)
        """
        if query is None:
            count = self.cache_manager.invalidate()
            logger.info(f"Invalidated all {count} cache entries")
        else:
            query_str = self._serialize_input(query)
            cache_key = self._generate_cache_key(query_str)
            count = self.cache_manager.invalidate(cache_key)
            logger.info(f"Invalidated {count} entries for query")
    
    def get_metrics(self) -> Dict[str, Any]:
        """
        Get performance metrics.
        
        Returns:
            Dictionary with metrics
        """
        cache_stats = self.cache_manager.get_stats()
        exec_stats = self.metrics.get_stats()
        
        return {
            # Cache metrics
            "cache_hit_rate": cache_stats["hit_rate"],
            "cache_hits": cache_stats["cache_hits"],
            "cache_misses": cache_stats["cache_misses"],
            "cache_size": cache_stats["current_size"],
            
            # Execution metrics
            "total_executions": exec_stats["total_executions"],
            "avg_cache_hit_latency": exec_stats["avg_cache_hit_latency"],
            "avg_execution_latency": exec_stats["avg_execution_latency"],
            
            # Cost metrics
            "estimated_cost": exec_stats["estimated_total_cost"],
            "estimated_cost_saved": exec_stats["estimated_cost_saved"],
            "cost_reduction_pct": exec_stats["cost_reduction_percentage"],
            
            # Feature status
            "hierarchical_enabled": self.config.enable_hierarchical,
            "compression_enabled": self.config.enable_compression,
        }
    
    def get_detailed_metrics(self) -> Dict[str, Any]:
        """Get detailed metrics with breakdown"""
        return {
            "cache": self.cache_manager.get_stats(),
            "execution": self.metrics.get_stats(),
            "circuit_breaker": {
                "enabled": self.config.enable_circuit_breaker,
                "status": self.circuit_breaker.state.value if self.circuit_breaker else "disabled",
                "failures": self.circuit_breaker.failure_count if self.circuit_breaker else 0
            },
            "config": {
                "similarity_threshold": self.config.similarity_threshold,
                "cache_ttl": self.config.cache_ttl,
                "max_cache_size": self.config.max_cache_size,
                "enable_hierarchical": self.config.enable_hierarchical,
                "enable_compression": self.config.enable_compression,
            }
        }
    
    def export_metrics(self, path: str):
        """Export metrics to JSON file"""
        metrics = self.get_detailed_metrics()
        
        with open(path, 'w') as f:
            json.dump(metrics, f, indent=2)
        
        logger.info(f"Exported metrics to {path}")
    
    def save_cache(self, path: str):
        """Save cache to disk"""
        self.cache_manager.save(path)
    
    def load_cache(self, path: str):
        """Load cache from disk"""
        self.cache_manager.load(path)


# ============================================================================
# PUBLIC API
# ============================================================================

def enhance(
    graph,
    config: Optional[OrchestraConfig] = None
) -> EnhancedLangGraph:
    """
    Enhance a LangGraph with Orchestra semantic caching.
    
    Usage:
        from langgraph.graph import StateGraph
        from orchestra import enhance
        
        graph = StateGraph(State)
        # ... add nodes, edges ...
        compiled = graph.compile()
        
        # Add Orchestra
        enhanced = enhance(compiled)
        
        # Use normally
        result = enhanced.invoke({"query": "..."})
    
    Args:
        graph: Compiled LangGraph instance
        config: Orchestra configuration
    
    Returns:
        Enhanced graph with semantic caching
    """
    return EnhancedLangGraph(graph, config)


__all__ = [
    "enhance",
    "EnhancedLangGraph",
    "OrchestraConfig",
]
